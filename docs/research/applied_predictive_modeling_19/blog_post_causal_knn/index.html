<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="utf-8">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Causal KNN</title>
  <meta name="author" content="" />

  
  <meta name="keywords" content="devows, hugo, go">	
  

  
  <meta name="description" content="Blog Post for Seminar Applied Predictive Analytics">
  

  <meta name="generator" content="Hugo 0.56.2" />

  <link href='//fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,500,700,800' rel='stylesheet' type='text/css'>

  
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  
  <link href="https://humboldt-wi.github.io/blog/css/animate.css" rel="stylesheet">

  
  
    <link href="https://humboldt-wi.github.io/blog/css/style.blue.css" rel="stylesheet" id="theme-stylesheet">
  


  
  <link href="https://humboldt-wi.github.io/blog/css/custom.css" rel="stylesheet">

  
  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
  <link rel="shortcut icon" href="https://humboldt-wi.github.io/blog/img/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" href="https://humboldt-wi.github.io/blog/img/apple-touch-icon.png" />
  

  <link href="https://humboldt-wi.github.io/blog/css/owl.carousel.css" rel="stylesheet">
  <link href="https://humboldt-wi.github.io/blog/css/owl.theme.css" rel="stylesheet">

  <link rel="alternate" href="https://humboldt-wi.github.io/index.xml" type="application/rss+xml" title="Institute of Infomation Systems at HU-Berlin">

  
  <meta property="og:title" content="Causal KNN" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="/blog/research/applied_predictive_modeling_19/blog_post_causal_knn//" />
  <meta property="og:image" content="img/logoGross.png" />

</head>


  <body>

    <div id="all">

        <header>

          <div class="navbar-affixed-top" data-spy="affix" data-offset-top="200">

    <div class="navbar navbar-default yamm" role="navigation" id="navbar">

        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand home" href="https://humboldt-wi.github.io/blog/">
                    <img src="https://humboldt-wi.github.io/blog/img/logo.png" alt="Causal KNN logo" class="hidden-xs hidden-sm">
                    <img src="https://humboldt-wi.github.io/blog/img/logo-small.png" alt="Causal KNN logo" class="visible-xs visible-sm">
                    <span class="sr-only">Causal KNN - go to homepage</span>
                </a>
                <div class="navbar-buttons">
                    <button type="button" class="navbar-toggle btn-template-main" data-toggle="collapse" data-target="#navigation">
                      <span class="sr-only">Toggle Navigation</span>
                        <i class="fa fa-align-justify"></i>
                    </button>
                </div>
            </div>
            

            <div class="navbar-collapse collapse" id="navigation">
                <ul class="nav navbar-nav navbar-right">
                  
                  <li class="dropdown">
                    
                    <a href="/blog/">Home</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/news/">News</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/contributors/">Contributors</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/research/">research</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/contact/">Contact</a>
                    
                  </li>
                  
                </ul>
            </div>
            

            <div class="collapse clearfix" id="search">

                <form class="navbar-form" role="search">
                    <div class="input-group">
                        <input type="text" class="form-control" placeholder="Search">
                        <span class="input-group-btn">

                    <button type="submit" class="btn btn-template-main"><i class="fa fa-search"></i></button>

                </span>
                    </div>
                </form>

            </div>
            

        </div>
    </div>
    

</div>




        </header>

        <div id="heading-breadcrumbs">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h1>Causal KNN</h1>
            </div>
        </div>
    </div>
</div>


        <div id="content">
            <div class="container">

                <div class="row">

                    

                    <div class="col-md-9" id="blog-post">

                        <p class="text-muted text-uppercase mb-small text-right">By <a href="#">Maximilian Kricke, Tim Peschenz</a> | August 1, 2019</p>

                        <div id="post-content">
                          

<h1 id="applied-predictive-analytics-seminar-causal-knn">Applied Predictive Analytics Seminar - Causal KNN</h1>

<h2 id="abstract">Abstract</h2>

<p>Beyond estimating the overall effect of a treatment, the uplift, econometric and statistical literature have set their eyes on estimating the personal treatment effect for each individual. This blogpost highly relates to the paper of Hitsch &amp; Misra (2018), where an uplift modeling approach is introduced, called Causal KNN. The k-nearest neighbor algorithm provides an interesting opportunity for the estimation of treatment effects in small groups. <!--Additional tweaks to the parameter tuning procedure make the approach to an interesting case study in the extension of machine learning concepts to causal modeling in general.--> The Causal KNN algorithm provides a simple and effective framework for the direct estimation of the Conditional Average Treatment Effect (CATE). Furthermore, the Transformed Outcome, described in Athey and Imbens (2015b), is used for the parameter tuning and evaluation of Uplift Models. The Causal KNN Algorithm is presented in an application framework, using a real world data set from an E-Mail marketing campaign.</p>

<h2 id="motivation-max">Motivation(Max):</h2>

<ul>
<li>Anwendungsbereiche &ldquo;Treatment Effect&rdquo;</li>
<li>Use Case: &ldquo;Business Evaluation&rdquo;:

<ul>
<li>Potential Outcome Framework (Rubin 2015)</li>
<li>Targeting Cost Discussion: Klare Abgrenzung von der Diskussion der Targeting Cost (-&gt; Verweis auf entsprechende Literatur)</li>
<li>Micro-Marketing Context (due to computational restrictions)</li>
</ul></li>
</ul>

<h2 id="summary-of-the-literature-foundation-tim">Summary of the Literature Foundation (Tim)</h2>

<p>This blogpost has strong relations to the work of Hitsch &amp; Misra (2018). A key goal of their paper is to compare many different estimates of different optimal targeting policies, using the inverse probability weighted profit estimator.</p>

<p>Causal Inference is done in the potential outcome framework. This framework will be briefly explained in the following. Assume a company is about to run an E-Mail campaign. Therefore, the company needs to decide on the amount of people that will receive an E-Mail. This decision depends on the cost of the campaign compared to the expected revenue. On a customer level, this decision boils down from the comparison of the targeting effort cost to the incremental profit contribution of each customer. The fundamental problem of treatment effect analysis is, that it can not be observed, whether the customers action was influenced by the received targeting or not. It can not be observed how a treated customer would have acted when he or she would not have received such an E-Mail. Therefore we aim at identifying the underlying factors of the customers behaviour, to eliminate these effects for the calculation of the treatment effect estimation. A given targeting policy cannot be directly evaluated in a randomized sample, because for many customers the proposed treatment assignment by the targeting policy will differ from the realized treatment assignment. However, for units where both, the proposed and realized treatment assignment agree, we can scale the realized profit contribution by the inverse of the propensity score to account for the percentage of similar units that are &ldquo;missing&rdquo;, in the sense that the proposed and realized treatment assignment are different. This problem can be described via the following graphic:</p>

<p><img src="../../../docs/img/seminar/causal_knn/cate_channel.png" alt="Classification of Customers according to Treatment and Reaction" /></p>

<p><img src="C:/Users/timpe_000/Desktop/blog/img/seminar/causal_knn/cate_channel.png" alt="Test" /></p>

<p><img align="center"
     style="display:block;margin:0 auto;"
     src="/blog/img/seminar/causal_knn/treatment_matrix.png"></p>

<p><img align="center"
     style="display:block;margin:0 auto;"
     src="/blog/img/seminar/image_analysis/bagOfWords.png"></p>

<p>C:\Users\timpe_000\Desktop\blog\docs\img\seminar\causal_knn</p>

<p>The aim of this paper is to explain estimation methods that directly predict the individual incremental effect of targeting.</p>

<p>To evaluate the treatment effect estimations, there are multiple approaches in the marketing literature (cite!). The observed data set consists of the observations <img src="http://chart.googleapis.com/chart?cht=tx&chl=$D = (Y_i, X_i, W_i)^N$" style="border:none;">, where <img src="http://chart.googleapis.com/chart?cht=tx&chl=$N$" style="border:none;"> is the number of customers, <img src="http://chart.googleapis.com/chart?cht=tx&chl=$Y_i$" style="border:none;"> is the target variable, <img src="http://chart.googleapis.com/chart?cht=tx&chl=$W_i$" style="border:none;"> the binary treatment variable (0,1) and <img src="http://chart.googleapis.com/chart?cht=tx&chl=$X_i$" style="border:none;"> represents the covariates vor each unit.</p>

<ul>
<li>Average Treatment effect: <img src="http://chart.googleapis.com/chart?cht=tx&chl=$E(Y_i(1)-Y_i(0)$" style="border:none;"></li>
<li>Average Treatment effect on the treated: <img src="http://chart.googleapis.com/chart?cht=tx&chl=$E(Y_i(1)-Y_i(0) | W_i = 1)$" style="border:none;"></li>
<li>Conditional Average Treatment effect: <img src="http://chart.googleapis.com/chart?cht=tx&chl=$E(Y_i(1)-Y_i(0) | X_i=x)$" style="border:none;"></li>
</ul>

<p>where <img src="http://chart.googleapis.com/chart?cht=tx&chl=$Y_i(1)$" style="border:none;"> and <img src="http://chart.googleapis.com/chart?cht=tx&chl=$Y_i(0)$" style="border:none;"> is the observed outcome for the treated (<img src="http://chart.googleapis.com/chart?cht=tx&chl=$W_i=1$" style="border:none;">) and the untreated (<img src="http://chart.googleapis.com/chart?cht=tx&chl=$W_i=0$" style="border:none;">) customers and <img src="http://chart.googleapis.com/chart?cht=tx&chl=$X_i$" style="border:none;"> is the vector of observed customer features. According to Hitsch and Misra (2018) an optimal policy targets a customer if and only if <img src="http://chart.googleapis.com/chart?cht=tx&chl=$\mathbb{E}[\pi(1)|X_i]>\mathbb{E}[\pi(0)|X_i]$&rdquo; style=&ldquo;border:none;&ldquo;&gt;, which means that the expected profit in case of targeting exceeds the expected profit in case the customer i is not targeted. This expression can be reformulated to:</p>

<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=$\mathbb{E}[\pi(1)-\pi(0)|X_i] > 0$&rdquo; style=&ldquo;border:none;&ldquo;&gt;</p>

<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=$\Longleftrightarrow \mathbb{E}[(mY_i(1))-c)-Y_i(0)|X_i]$" style="border:none;"></p>

<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=$\Longleftrightarrow \mathbb{E}[Y_i(1)-Y_i(0)|X_i]-c >0$&rdquo; style=&ldquo;border:none;&ldquo;&gt;</p>

<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=$\Longleftrightarrow$" style="border:none;"><img src="http://chart.googleapis.com/chart?cht=tx&chl=$m\tau(x) < c$" style="border:none;"></p>

<p>Here, <img src="http://chart.googleapis.com/chart?cht=tx&chl=$\tau(x)$" style="border:none;"> is the conditional average treatment effect (CATE): <img src="http://chart.googleapis.com/chart?cht=tx&chl=$\tau(x)=E(Y_i(1)-Y_i(0) | X_i=x)$" style="border:none;">. The CATE describes the average causal effect of targeting for a sub-population of customers, with identical features. It is shown in Hitsch, Misra (2018), that the CATE is sufficient to identify an optimal targeting policy, but generally the CATE can not be inferred from the data. To account for the fundamental problem of unobservable individual treatment effects, Hitsch and Misra (2018) introduced three assumptions to the data, to be able to identify the CATE. If the data satisfies the following assumptions, the conditional average treatment effect is identifiable from the data.</p>

<ol>
<li>Unconfoundedness: <img src="http://chart.googleapis.com/chart?cht=tx&chl=$Y_i(0), Y_i(1)$" style="border:none;"><img src="http://chart.googleapis.com/chart?cht=tx&chl=$\bot$" style="border:none;"><img src="http://chart.googleapis.com/chart?cht=tx&chl=$W_i | X_i$" style="border:none;">
It means, that ones individuals outcome does not depend on the treatment conditional on X. &lsquo;The treatment effect is identical within a subset of customers with identical features.&rsquo; This condition can be fulfilled by using a randomized sample set. In marketing, it is often possible to address campaigns randomized, therefore this condition should be satisfied in general.</li>
<li>The Overlap Assumption: <img src="http://chart.googleapis.com/chart?cht=tx&chl=$0 < e(x) < 1$" style="border:none;">. The propensity score <img src="http://chart.googleapis.com/chart?cht=tx&chl=$e(x)$" style="border:none;"> is the probability of being targeted, conditional on the customers features and can be translated as the &lsquo;Targeting Probability&rsquo; for each unit. This targeting probabilty has to be defined strictly between zero and one.</li>
<li>Stable Unit Treatment Value Assumption (SUTVA): The treatment received by customer i, <img src="http://chart.googleapis.com/chart?cht=tx&chl=$W_i$" style="border:none;">, has no impact on the behavior of any other customer. There are no social interaction or equilibrium effects among the observed units. According to Hitsch (2018), the SUTVA assumption should be satisfied in general. It appears implausible to observe economically significant social effects due to the receipt of e.g. a catalog.<br /></li>
</ol>

<p>If those three assumptions are satisfied, we obtain the following formula to calculate the &lsquo;true&rsquo; conditional average treatment effect:
<img src="http://chart.googleapis.com/chart?cht=tx&chl=$\tau(x) = \mathbb{E}[Y_i|X_i,W_i = 1] - \mathbb{E}[Y_i|X_i,W_i = 0]$" style="border:none;"></p>

<h3 id="identification-of-conditional-average-treatment-effects">Identification of conditional average treatment effects</h3>

<p>It is still unrealistic to assume that we have more than one customer with exactly the same feature set, especially in high dimensional data with many feature variables. We can find this sitiuation especially in big data applications, where units are characterized with a vast amount of features. Therefore we need to account for the heterogeniety among the customers in the data set. The proposed approach to identify thse observations, which are similar to each other, is k-nearest neighbours.</p>

<p>To estimate the conditional average treatment effect, we will use a direct approach. Such methods appear to be infeasible since we are restricted by the fundamental problem of causal inference. Having the fundamental problem in mind, we will discuss an estimation method to approximate the unobservable CATE. It is genrally considered, that the conditional average treatment effect cannot be inferred from the data. Nevertheless, using a combination of knn-Clustering and the transformed outcome approach, we are able to build an estimate that is asymptotically equivalent and unbiased to the &ldquo;true&rdquo; CATE.</p>

<p>The proposed approach from Hitsch and Misra (2018) allows to evaluate the profit of several different targeting policies, by using only one single randomized sample. This offers the opportunity to compare various targeting policies, without the need for multiple field experiments. According to Hitsch and Misra (2018), an optimal targeting policy has to concentrate on the binary treatment case, where every unit is either treated or not and that a unit is only treated, if the incremental value of the treatment exceeds the targeting costs. Hitsch and Misra (2018) introduce their approach as a direct estimation method. Direct estimation methods focus on measuring and predicting the treatment effect of certain actions. This is based on the squared difference between the true and the estimated treatment effect for each unit i: <img src="http://chart.googleapis.com/chart?cht=tx&chl=$\mathbb{E}[(\tau(X_i)-\hat{\tau}(X_i))^2]$" style="border:none;">. Opposed to direct estimation approaches, where the focus lies on estimating the treatment effect itself, indirect estimation methods concentrate on the observed and the predicted outcome levels. To dicide for an optimal targeting policy, the squared error loss between the two outcome levels are minimized: <img src="http://chart.googleapis.com/chart?cht=tx&chl=$\mathbb{E}[(Y_i-\hat{\mu}(X_i,W_i))^2]$" style="border:none;">. Therefore the indirect approaches focus on the outcome levels instead on the treatment effect itself. Hitsch and Misra (2018) describe the indirect approaches as conceptually wrong in the field of uplift estimation. They recommend the usage of direct estimation methods, since they yield at more accurate estimations of the treatment effect. The Causal KNN estimation, combined with the transfomed outcome, can be used to derive an approximation of the CATE for each observation in a data set.</p>

<h2 id="the-causal-knn-algorithm-tim">The Causal KNN Algorithm(Tim)</h2>

<ul>
<li>Algorithm</li>
<li>Application: Visit (Spend auch möglich, aber zu unbalanced Data)</li>
<li>Implementation (Vorstellung &ldquo;FNN-Package&rdquo;)</li>
<li>Einordnung als Two-Model-Approach &amp; Direct Estimation Method (vgl. Hitsch)
<br /></li>
</ul>

<h3 id="causal-knn-estimation">Causal KNN Estimation</h3>

<p>As mentioned before, the true CATE is not observable. Hitsch and Misra use a method to approximate the actual treatment effect via the transformed outcome Y*. To overcome the fundamental problem of treatment effect estimation, the Causal KNN algorithm is used. This approach allows to derive an estimation of the CATE on an individual unit level. The Causal KNN approach is described as a direct estimation method.
The algorithm first starts with selecting the k treated and untreated nearest neighbours for each observation. The nearest neighbours are selected, based on the covariates, using the euclidean distance measure. However, every other distnce measure would be similarily feasible, depending on the application framework. Afterwards, the individual treatemnt effects are derived by computing the differences between the mean outcome values of the treated and the untreated nearest neighbours for all units. This yields at an estimation of the CATE for each observation in the data set. This procedure is represented in the following equation: <img src="http://chart.googleapis.com/chart?cht=tx&chl=$\hat{\tau}_K(x)= \frac{1}{K} \sum_{i \in N_K(x,1)} Y_i - \frac{1}{K}  \sum_{i \in N_K(x,0)} Y_i$" style="border:none;">, where <img src="http://chart.googleapis.com/chart?cht=tx&chl=$Y_i$" style="border:none;"> represents the outcome values of the target variable and <img src="http://chart.googleapis.com/chart?cht=tx&chl=$N_K(x,0)$" style="border:none;"> and <img src="http://chart.googleapis.com/chart?cht=tx&chl=$N_K(x,1)$" style="border:none;"> denote the nearest neighbour units with treatment status <img src="http://chart.googleapis.com/chart?cht=tx&chl=$W_i = 1$" style="border:none;"> and <img src="http://chart.googleapis.com/chart?cht=tx&chl=$W_i = 0$" style="border:none;"> respectively. Since the Causal KNN algorithm takes the number of nearest neighbours k as a freely selectable parameter, it is necessary to find a way to choose the optimal k value.
<!-- These methods select the tuning parameter K using a feasible loss that, in expectation, attains its minimum at the same K value as the infeasible loss based on the treatment effect. 
For any vector x, we find the K nearest *treated* neighbors and separately the K nearest *untreated* neighbors. We then estimate the CATE using the difference between the nearest treated and untreated units:
$\widehat{\tau_K}(x)= \frac{1}{K} \sum_{i \in N_K(x,1)} Y_i - \frac{1}{K}  \sum_{i \in N_K(x,0)} Y_i$.--></p>

<h3 id="comparison-of-causal-knn-and-the-two-model-approach">Comparison of Causal KNN and the Two-Model-Approach</h3>

<p>According to Rzepakowski and Jaroszewicz (<sup>2012</sup>&frasl;<sub>2</sub>), the idea behind the two model approach is to build two separate models to estimate the treatment effect. One Model is trained, using the treatment, and the other one using the control data set. After building the two models, the treatment effect estimations are calculated by subtracting the predicted class probabilities from the control model from those of the treatment model. This yields at a direct estimation of the difference in the outcome, caused by the treatment. The advantage of the two model approach is that it can be applied with any classification model (Rzepakowski and Jaroszewicz, 2012). A possible problem that might arise with this approach, is that the uplift might be differently from the class distributions. In this case, the models focus on predicting the class, rather than the uplift.
The Causal KNN ALgorithm also calculates the difference between the predicted outcomes of the treated and untreated observations in a data set. The number of observations that are taken into account, is limited by the KNN parameter K. Therefore, the Causal KNN algorithm itself can be possibly seen as a special case of the two model approach. An important aspect of the described method here, is the usage of the transformed outcome, for the tuning of the causal KNN model. Since the combination of causal KNN and the transformed outcome allows to calculate the CATE estimations directly, it is not appropriate to completely assign this to the group of two model approaches. According to Rzepakowski and Jaroszewicz (<sup>2012</sup>&frasl;<sub>2</sub>), the two model approach is different from the direct estimation methods, since it focuses on predicting the outcome of the target variable to estimate the treatment effect, rather than predicting the treatment effect itself. As already shown above, the Causal KNN algorithm should be rather categorized as a direct estimation approach (Hitsch and Misra, 2018) and should therefore not be seen as a two model approach.</p>

<h2 id="data-presentation">Data Presentation</h2>

<p>For the application of the presented methodology, data from an E-Mail targeting campaign was used, including 64,000 observations. The data entries represent customer transactions with serveral corresponding variables. The treatment variable &ldquo;segment&rdquo; originally includes three possible values, E-Mail for Women, E-Mail for Men and No E-Mail. There are three different target variables. &ldquo;Visit&rdquo; as the visitation of the companies website and &ldquo;Conversion&rdquo; of the customer are binary variables. The numeric variable &ldquo;Spend&rdquo; represent the amount of money spend by a customer in the following two weeks after the campaign.
There are various covariates that characterize customers according to their personal properties and purchasing behaviour. The &ldquo;Recency&rdquo; represents the month since the last purchase was made. &ldquo;History&rdquo; and &ldquo;History_Segment&rdquo; include information about the amount of money spend in the last year, in dollars. &ldquo;Mens&rdquo; and &ldquo;Womens&rdquo; are binary variables that indicate whether a customer bought products for men or for  women. The &ldquo;Zip_Code&rdquo; includes geographical information, grouped into Urban, Suburban and Rural regions. &ldquo;Newbie&rdquo; states whether a person was registered as a new customer in the past 12 month and the Channel represents on which ways a customer placed orders, i.e. via phone, mail or both. To provide a comprehensive overview about the data, an example table is presented, including first observations of the data set.</p>

<pre><code class="language-r">###load data
library(readr)
data = read_csv(&quot;../Data/mail_data.csv&quot;)

head(data)
</code></pre>

<table>
<thead>
<tr>
<th>recency</th>
<th>history_segment</th>
<th>history</th>
<th>mens</th>
<th>womens</th>
<th>zip_code</th>
<th>newbie</th>
<th>channel</th>
<th>segment</th>
<th>visit</th>
<th>conversion</th>
<th>spend</th>
<th>idx</th>
<th>treatment</th>
</tr>
</thead>

<tbody>
<tr>
<td>10</td>
<td>2) $100 - $200</td>
<td>142.44</td>
<td>1</td>
<td>0</td>
<td>Surburban</td>
<td>0</td>
<td>Phone</td>
<td>Womens E-Mail</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>

<tr>
<td>6</td>
<td>3) $200 - $350</td>
<td>329.08</td>
<td>1</td>
<td>1</td>
<td>Rural</td>
<td>1</td>
<td>Web</td>
<td>No E-Mail</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>0</td>
</tr>

<tr>
<td>7</td>
<td>2) $100 - $200</td>
<td>180.65</td>
<td>0</td>
<td>1</td>
<td>Surburban</td>
<td>1</td>
<td>Web</td>
<td>Womens E-Mail</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>1</td>
</tr>

<tr>
<td>9</td>
<td>5) $500 - $750</td>
<td>675.83</td>
<td>1</td>
<td>0</td>
<td>Rural</td>
<td>1</td>
<td>Web</td>
<td>Mens E-Mail</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
<td>1</td>
</tr>

<tr>
<td>2</td>
<td>1) $0 - $100</td>
<td>45.34</td>
<td>1</td>
<td>0</td>
<td>Urban</td>
<td>0</td>
<td>Web</td>
<td>Womens E-Mail</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
<td>1</td>
</tr>

<tr>
<td>6</td>
<td>2) $100 - $200</td>
<td>134.83</td>
<td>0</td>
<td>1</td>
<td>Surburban</td>
<td>0</td>
<td>Phone</td>
<td>Womens E-Mail</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>6</td>
<td>1</td>
</tr>
</tbody>
</table>

<h3 id="data-preparation">Data Preparation</h3>

<pre><code class="language-r,">#convert to data frame
data = as.data.frame(data)
str(data)
</code></pre>

<p>After importing the e-mail data set, the non-numeric variables are converted into factors. Afterwards, all columns are examined with regard to missing values. There are no missing observations in the data set. Therefore, an imputation of missing values is not necessary for none of the variables.</p>

<pre><code class="language-r,">###data type
#history_segment as factor
data$history_segment = as.factor(data$history_segment)
levels(data$history_segment)

table(data$history_segment)

#zip code as factor
data$zip_code = as.factor(data$zip_code)
levels(data$zip_code)

table(data$zip_code)

#channel as factor
data$channel = as.factor(data$channel)
levels(data$channel)

table(data$channel)

#segment as factor
data$segment = as.factor(data$segment)
levels(data$segment)

table(data$segment)


###check for NA values in the data set
sapply(data[,1:ncol(data)], FUN = function(x){sum(is.na(x))})
</code></pre>

<p>To label the different observations in the data set, an index column was added, in order to be able to reassign all observations in the later steps of the analysis. The original data set contains three outcomes for the treatment variable, i.e. E-Mail for Women, E-Mail for Men and no E-Mail. Since the analysis is designed to deal with binary treatment variables, the three possible treatment values are reduced to either receiving an E-Mail or not. For the further process of the analysis, the binary treatment variable indicates solely whether units received a treatment. There is no distinction anymore between E-Mails for Women and Men. Since the different treatments were randomly assigned with the same probability, the aggregation of the treatment yields at a propensity score of <sup>2</sup>&frasl;<sub>3</sub> for each individual. Therefore, the treatment assignment in the data set is slightly unbalanced. Since the difference between the number of treated and untreated customers is not too high, the unbalancedness should not cause any problems for the proposed method. For extremely unbalanced data, the causal KNN algorithm might not work, because it might be impossible to find sufficiently much treated or untreated nearest neighbours.
The KNN algortihm picks the nearest neighbours by calculating the euclidean distances between target variable outcomes of different units. Therefore, the method can only deal with numeric variables. To meet this requirement, the non-numeric covariates are transformed into dummy variables, using the mlr package.</p>

<pre><code class="language-r,">###preparation of the data set
#adding index column
data$idx = seq(1:nrow(data))

#adding binary treatment column
data$treatment = sapply(data$segment, FUN = function(x) ifelse(x == &quot;No E-Mail&quot;, 0, 1))

#create dummy features
library(mlr)

data_d = createDummyFeatures(data, target = &quot;visit&quot;)
colnames(data_d)
</code></pre>

<h3 id="causal-knn-algorithm-implementation">Causal KNN Algorithm Implementation</h3>

<p>The Causal KNN alorithm starts with selecting the k treated and untreated nearest neighbour observations for each unit i. To allow a simple selection of both groups, the data set is separated into two data frames, containing the treated and untreated observations respectively.</p>

<pre><code class="language-r,">#select data partition for training
data_d = data_d[1:10000, ]

###splitting data set
#1 no treatment
data_nt = data_d[which(data_d$segment.No.E.Mail == 1),]

#2 treatment
data_t = data_d[which(data_d$segment.No.E.Mail == 0),]

#checking target variables
sum(data_d$visit != 0)
sum(data_d$conversion != 0)
sum(data_d$spend != 0)
</code></pre>

<p>The KNN search is realized by using the FNN package in R. This package allows to extract the index values of the nearest neighbours for each observation and the corresponding distances. An important aspect of causal KNN is to find the optimal k value for a specific application setting. To check for several values of k, it is necessary to specify the maximum number of nearest neighbours that is going to be tested. The possible k value is restricted by the minimum total number of treated or untreated observations, i.e. the minimum number of rows of the data frames with the two treatment groups. A very useful feature of the FNN package, is the possibility to specify different data sets for the search space and the observations, for which nearest neighbours have to be found. This allows to conduct an simple search for nearest neighbours among the treated and untreated observations. There are two data frames resulting from both steps of the search process. The treated_nn data frame includes the nearest neighbours with treatment status 1 and untreated_nn those with treatment status 0. These two data sets build the basis for the next step of the algorithm, i.e. the identification of the optimal k value.</p>

<pre><code class="language-r,">###Causal KNN
#install.packages(&quot;FNN&quot;)
library(FNN)

#setting parameter k for number of nearest neighbours
k = 3000
k = k + 1

#select columns to be eliminated for nearest neighbour search
drop_cols = c(&quot;visit&quot;, 
              &quot;spend&quot;, 
              &quot;conversion&quot;, 
              &quot;idx&quot;, 
              &quot;treatment&quot;, 
              &quot;segment.Mens.E.Mail&quot;,
              &quot;segment.Womens.E.Mail&quot;,
              &quot;segment.No.E.Mail&quot;)

#garbage collection to maximize available main memory
gc()

#available memory im Mb
memory.limit()

#set memory limit
#memory.limit(size = 50000)

#calculate indices of k treated nearest neighbours
treated_nn = get.knnx(data_t[, !(names(data_t) %in% drop_cols)], 
                      query = data_d[, !(names(data_d) %in% drop_cols)], 
                      k = k)
treated_nn = data.frame(treated_nn$nn.index)

#deleting first column
treated_nn = treated_nn[, 2:ncol(treated_nn)]

treated_nn[1:10, 1:10]

#calculate indices of k untreated nearest neighbours
untreated_nn = get.knnx(data_nt[, !(names(data_nt) %in% drop_cols)], 
                        query = data_d[, !(names(data_d) %in% drop_cols)], 
                        k = k)
untreated_nn = data.frame(untreated_nn$nn.index)

#deleting first column
untreated_nn = untreated_nn[, 2:ncol(untreated_nn)]

untreated_nn[1:10, 1:10]

#checking results
data_d$treatment[data_t$idx[unlist(treated_nn[1,][1:30])]]
data_d$treatment[data_nt$idx[unlist(untreated_nn[1,][1:30])]]


#replacing index values
treated_nn = sapply(treated_nn, FUN = function(x){
  data_t$idx[x]
})

untreated_nn = sapply(untreated_nn, FUN = function(x){
  data_nt$idx[x]
}) 

treated_nn[1:10, 1:10]
untreated_nn[1:10, 1:10]

#transpose data frames
treated_nn = t(treated_nn)
untreated_nn = t(untreated_nn)
</code></pre>

<p>To estimate a robust CATE, we would likely use all customers in the neighborhood. A high value for K increases the accounted neighborhood, while it decreases the similarity between the customers. Therefore, the estimation depends on the choice of K. To find an optimal value of K, we want to minimize the squared difference between <img src="http://chart.googleapis.com/chart?cht=tx&chl=$\widehat{\tau_K}(x)$" style="border:none;"> and the transformed outcome Y*. This is called the &lsquo;Transformed Outcome Approach&rsquo;, which will be described in the following.</p>

<h2 id="transformed-outcome-approach">Transformed Outcome Approach</h2>

<p>Vorstellung &ldquo;Transformed Outcome Approach&rdquo; (Max)
Rechtfertigung (Mathematisch + Intuitiv)</p>

<ul>
<li>&ldquo;Transformed Outcome Approach&rdquo; (Max)

<ul>
<li>Justification</li>
</ul></li>
</ul>

<p>In order to tune the parameter k, we are using the transformed outcome loss, which is a proxy for the true conditional treatment effect. The transformed outcome is defined as:</p>

<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=$Y_i^*=W_i*\frac{Y_i(1)}{e(X_i)}-(1-W_i)*\frac{Y_i(0)}{1-e(X_i)}$" style="border:none;"></p>

<p>Since it only depends on the potential outcome that corresponds to the realized treatment level, the transformed outcome can be calculated from the observed outcome <img src="http://chart.googleapis.com/chart?cht=tx&chl=$Y_i$" style="border:none;">:</p>

<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=$Y_i^*=\frac{W_i - e(X_i)}{e(X_i)(1-e(X_i))}Y_i$" style="border:none;"></p>

<p>Since we are having a formula for the predicted outcome, we still need to decide on the actual outcome.
For a randomized sample, the propensity score <img src="http://chart.googleapis.com/chart?cht=tx&chl=$e(x)$" style="border:none;"> is not relevant, since it is accounted in every individuals treatment effect in the same manner.
It can be shown that, under unconfoundedness, the transformed outcome is an unbiased estimator for the actual treatment effect.</p>

<h3 id="intuitive-interpretation-of-the-transformed-outcome">Intuitive Interpretation of the transformed Outcome</h3>

<h3 id="intuitive-interpretation-of-the-mean-squared-error-loss">Intuitive Interpretation of the mean squared error loss</h3>

<h3 id="explanation-why-an-optimal-choice-of-k-is-sufficient">Explanation, why an optimal choice of K is sufficient</h3>

<h3 id="parameter-tuning-using-the-transformed-outcome">Parameter Tuning using the Transformed Outcome</h3>

<p>An essential part of the causal KNN algorithm is the parameter tuning to specify the k value, that leads to the most accurate treatment effect estimations. For the parameter tuning, several values for k are tested and different CATE estimations are stored in a separate data frame. To derive estimations for all observations in the data set, the differences of the mean outcome variable values of the k treated and untreated nearest neigbours are calculated. Several iterations of this precedure yield at a data frame, containing the individual uplift estimations for different values of k. To select the most accurate estimation, the transformed outcome is used to decide for a preferable k value.</p>

<pre><code class="language-r,">###parameter tuning to find optimal k value
#setting parameters for the number of neigbours
k_start = 100
k_end = 3000
steps = 100

#creating sequence of k values to test for
k_values = seq(from = k_start, to = k_end, by = steps)
length(k_values)

#preparing uplift data frame
uplift = data.frame(&quot;idx&quot; = data_d$idx,
                    &quot;treatment&quot; = data_d$treatment)

#calculating uplift for specified k values
for (k in k_values) {
  reaction_nt = apply(untreated_nn, MARGIN = 2, 
                      FUN = function(x){
                        mean(data_d$visit[x[1:k]])
                        }
                      )
  
  reaction_t = apply(treated_nn, MARGIN = 2, 
                     FUN = function(x){
                       mean(data_d$visit[x[1:k]])
                       }
                     )
  
  uplift[paste(&quot;uplift_&quot;,k, sep = &quot;&quot;)] = reaction_t - reaction_nt
  
  print(paste(&quot;k = &quot;, k))
}

#check result
uplift[1:10, ]
</code></pre>

<h2 id="transformed-outcome-loss-calculation">Transformed Outcome Loss Calculation</h2>

<p>After calculating the various uplift estimations by varying the tuning parameter k, the transformed outcome, as proxy for the &ldquo;true&rdquo; CATE, is constructed. The calculation of the transformed outcome needs the propensity score, the treatment status and the original value of the target variable. By using the formula introduced in the theory section, the transformed outcome can be easily calculated for the actual target variabe visit. To decide on a value of k, the transformed outcome loss is calculated for each potential k value, by computing the mean squared difference between the transformed outcome and the various treatment effect estimations. The estimation that yields at the minimal value of the transformed outcome loss indicates the most accurate value for k, as shown by Hitsch and Misra (2018). The following plot was created to depict the development of the outcome loss with varying k values. When increasing the number of nearest neighbour observations that are taken into account, the accuracy of the CATE estimation increases in the beginning. However, when increasing k over a certain level, the heterogeniety among the nearest neighbours increases to strongly. That leads to less accurate estimations of the treatment effects.</p>

<pre><code class="language-r,">###transformed outcome
#propensity score
e_x = mean(data_d$treatment)
e_x

#function for transformed outcome
transformed_outcome = function(e_x, w_i, target){
  return(((w_i - e_x)/(e_x*(1-e_x)))*target)
}

#apply function on all observations
trans_out = sapply(uplift$idx, FUN = function(x){
  transformed_outcome(e_x = e_x, 
                      w_i = data$treatment[x], 
                      target = data$visit[x])
})

trans_out[1:50]
data$treatment[1:20]
data$visit[1:20]

uplift$trans_out = trans_out
rm(trans_out)

#transformed outcome loss
outcome_loss = data.frame(&quot;k&quot; = k_values, &quot;loss&quot; = 0)

#find optimal k value from transformed outcome loss
for (i in 1:length(k_values)){
  outcome_loss[i, 2] = mean((uplift$trans_out - 
                               uplift[, i + 2])^2)
}

outcome_loss

#find minimal outcome loss value
min(outcome_loss$loss)

outcome_loss$k[which.min(outcome_loss$loss)]

plot(outcome_loss$loss, type=&quot;b&quot;)

#plot result
library(ggplot2)

k_plot = ggplot(data = outcome_loss) +
  geom_point(aes(x = outcome_loss$k, 
                 y = outcome_loss$loss), 
             size = 2, shape = 18) +
  geom_point(aes(x = outcome_loss$k[which.min(outcome_loss$loss)], 
                 y = min(outcome_loss$loss)), 
             size = 4, shape = 18, color = &quot;red&quot;) +
  geom_text(aes(x = outcome_loss$k[which.min(outcome_loss$loss)], 
                y = min(outcome_loss$loss)), 
            label = paste(&quot;K = &quot;, outcome_loss$k[which.min(outcome_loss$loss)]), 
            color = &quot;black&quot;, size = 4, nudge_x = 120, check_overlap = TRUE) +
  stat_smooth(data = outcome_loss, 
              aes(x = outcome_loss$k, y = outcome_loss$loss), 
              method = &quot;loess&quot;, se = FALSE, span = 0.1) +
  labs(title=&quot;Parameter Optimization of Causal-KNN CATE-Estimation&quot;, 
       x =&quot;Value of K&quot;, y = &quot;Outcome Loss Value&quot;) +
  theme_light()

k_plot
</code></pre>

<p><img src="Results/k_plot.png" alt="Outcome Loss for Different Values of K" /></p>

<p>After finding the most accurate estimation of the CATE, it is possible to extract valuable insights from the data. This information kann be used to plan targeting policies for future campaigns. Since the Causal KNN algorithm delivers treatment effect estimation on an individual level, it is easily possible to extract several other treatemnt effect measures. The average treatment effect (ATE) and the average treatment effect on the treated(ATT) can be derived. Furthermore, different CATE estimations based on different dimensions of the data can be calculated by grouping the observations.</p>

<pre><code class="language-r,">#extract best uplift estimation
data_d$uplift = uplift[, paste0(&quot;uplift_&quot;, outcome_loss$k[which.min(outcome_loss$loss)])]
data_d$uplift[1:20]

#ATE
ate = mean(data_d$uplift)
ate

#ATT
att = aggregate(data_d$uplift, by=list(data$treatment[1:nrow(data_d)]), FUN=mean)
att

#CATE
cate_sex = aggregate(data_d$uplift, by=list(data$mens[1:nrow(data_d)]), FUN=mean)
cate_sex

cate_hist_seg = aggregate(data_d$uplift, by=list(data$history_segment[1:nrow(data_d)]), FUN=mean)
cate_hist_seg

ggplot(data = cate_hist_seg, aes(x = Group.1, y = x)) +
  geom_bar(stat = &quot;identity&quot;, fill = &quot;#adc7db&quot;) +
  labs(title=&quot;CATE-Estimation for different History Segments&quot;, x =&quot;History Segment&quot;, y = &quot;CATE&quot;) +
  theme_light() +
  geom_text(aes(label = round(x, digits = 4)), vjust = 1.6, color = &quot;black&quot;, size = 3.5)

cate_zip_code = aggregate(data_d$uplift, by=list(data$zip_code[1:nrow(data_d)]), FUN=mean)
cate_zip_code

cate_newbie = aggregate(data_d$uplift, by=list(data$newbie[1:nrow(data_d)]), FUN=mean)
cate_newbie

ggplot(data = cate_newbie, aes(x = Group.1, y = x)) +
  geom_bar(stat = &quot;identity&quot;, fill = &quot;#adc7db&quot;) +
  labs(title=&quot;CATE-Estimation for different Newbie-Status&quot;, x =&quot;Newbie&quot;, y = &quot;CATE&quot;) +
  theme_light() +
  geom_text(aes(label = round(x, digits = 4)), vjust = 1.6, color = &quot;black&quot;, size = 3.5)

cate_channel = aggregate(data_d$uplift, by=list(data$channel[1:nrow(data_d)]), FUN=mean)
cate_channel

ggplot(data = cate_channel, aes(x = Group.1, y = x)) +
  geom_bar(stat = &quot;identity&quot;, fill = &quot;#adc7db&quot;) +
  labs(title=&quot;CATE-Estimation for different Communication Channels&quot;, x =&quot;Communication Channel&quot;, y = &quot;CATE&quot;) +
  theme_light() +
  geom_text(aes(label = round(x, digits = 4)), vjust = 1.6, color = &quot;black&quot;, size = 3.5)
</code></pre>

<p><img src="Results/cate_hist_seg.png" alt="CATE for History Segments" />
<img src="Results/cate_channel.png" alt="CATE for Channels" /></p>

<p>Another advantage of the causal KNN method is the high flexibility with regard to the target variable. As already shown, the algorithm works with a binary outcome variables, e.g. visit in this case. By simply exchanging the target variable with a numeric one (spend), the algorithm can be executed similarily, without further modifications to the code. The CATE estimations represent an approximation of the total uplift that is expected in the case of a treatment. Therefore, the CATE for the target variable spend indicates the expected absolute increase or deacrease of spendings for each individual, when a treatment would be assigned. The application of the Causal KNN algorithm for numeric target variables does not need further changes to the implementation, instead of changing the outcome variable from visit to spend. Therefore, the code is not provided here in the blogpost. A possible implementation can be seen here (LINK!). Although the data theoretically allow for an estimation of the CATE for the variable spend, the insights of that estimation are not that valuable, because the number of converted observations is very low. Therefore, the dataset is extremely unbalanced with regard to the variable spend.</p>

<h3 id="causal-knn-for-optimal-targeting-policy-prediction">Causal KNN for optimal Targeting Policy Prediction</h3>

<p>The next section presents the causal KNN method in an application case, where a targeting policy has to be developed, based on the CATE estimations. From the model training and parameter tuning of previous steps, it was possible to find a best fitting value for the number of nearest neighbours k, that are taken into account. This value is now used in the prediction setting, where we assume that we do not know the actual outcome values. The aim is to find those individuals, where a treatment increases the value of the outcome variable the most. By specifying a fixed percentage of the population, we are able to select the most profitable individuals for a treatment assignment. The causal KNN algorithm process is similar to the one above, but without any parameter tuning. The important difference to the causal KNN estimation for the k value determination, is the change in the search space. For the test data, we assume that we did not decide for a treatment yet and do not have any information about the individual outcome values. Therefore, we are not able to calculate the transformed outcome for the test data set. Since we decided in a previous step for an optimal parameter of k, we use this value here as parameter for the causal knn algorithm. The difference is now that we do not search for treated and untreated nearest neighbours in the test data set. We are not able to do so, because we did not decide for any treatment assignment yet. We rather search the neighbours for our observations in the training data set, which we already used for finding the optimal k value.</p>

<pre><code class="language-r,">###using causal knn as predictive model
test_set = createDummyFeatures(data[10001:20000, ])
test_set$idx = 1:nrow(test_set)

#splitting data set
#1 no treatment
data_nt = data_d[which(data_d$segment.No.E.Mail == 1),]

#2 treatment
data_t = data_d[which(data_d$segment.No.E.Mail == 0),]


#running causal knn for test set to calculate mse
#select target columns
drop_cols = c(&quot;visit&quot;, 
              &quot;spend&quot;, 
              &quot;conversion&quot;, 
              &quot;idx&quot;, 
              &quot;treatment&quot;, 
              &quot;segment.Mens.E.Mail&quot;,
              &quot;segment.Womens.E.Mail&quot;,
              &quot;segment.No.E.Mail&quot;,
              &quot;uplift&quot;)


#setting optimal k value from the parameter tuning above
k = outcome_loss$k[which.min(outcome_loss$loss)] + 1


#calculate indices of k treated nearest neighbours
treated_nn = get.knnx(data_t[, !(names(data_t) %in% drop_cols)], 
                      query = test_set[, !(names(test_set) %in% drop_cols)], 
                      k = k)
treated_nn = data.frame(treated_nn$nn.index)

#deleting first column
treated_nn = treated_nn[, 2:ncol(treated_nn)]

treated_nn[1:10, 1:10]


#calculate indices of k untreated nearest neighbours
untreated_nn = get.knnx(data_nt[, !(names(data_nt) %in% drop_cols)], 
                        query = test_set[, !(names(test_set) %in% drop_cols)], 
                        k = k)
untreated_nn = data.frame(untreated_nn$nn.index)

#deleting first column
untreated_nn = untreated_nn[, 2:ncol(untreated_nn)]

untreated_nn[1:10, 1:10]

#checking results
data_d$treatment[data_t$idx[unlist(treated_nn[1,][1:30])]]
data_d$treatment[data_nt$idx[unlist(untreated_nn[1,][1:30])]]

#replacing index values
treated_nn = sapply(treated_nn, FUN = function(x){
  data_t$idx[x]
})

untreated_nn = sapply(untreated_nn, FUN = function(x){
  data_nt$idx[x]
}) 

treated_nn[1:10, 1:10]
untreated_nn[1:10, 1:10]

#transpose data frames
treated_nn = t(treated_nn)
untreated_nn = t(untreated_nn)

#preparing uplift data frame
uplift = data.frame(&quot;idx&quot; = test_set$idx,
                    &quot;treatment&quot; = test_set$treatment)

#rownames(test_set) = 1:nrow(test_set)
#calculating uplift for specified k values

reaction_nt = apply(untreated_nn, MARGIN = 2, FUN = function(x){
  mean(data_d$visit[x[1:k-1]])
})

reaction_t = apply(treated_nn, MARGIN = 2, FUN = function(x){
  mean(data_d$visit[x[1:k-1]])
})

uplift[paste(&quot;uplift_&quot;, k, sep = &quot;&quot;)] = reaction_t - reaction_nt


#check result
uplift[1:10, ]
</code></pre>

<h2 id="application-of-transformed-outcome-approach-for-general-uplift-models">Application of Transformed Outcome Approach for general Uplift Models</h2>

<h3 id="causal-tree-model">Causal Tree Model</h3>

<p>The parameter tuning using the transformed outcome loss is also useful for other uplift applications. To show this, the same methodology that was applied to determine the optimal k value for the causal KNN model is here used to find optimal parameters for a causal tree. Different values for parameters are tested and evaluated by using the transformed outcome loss. The minimal outcome loss value again indicates the most favorable parameter value for the model estimation.</p>

<pre><code class="language-r,">#using transformed outcome for tuning parameters of other models
#different cp for causal tree

#install.packages(&quot;devtools&quot;)
#library(devtools) 
#install_github(&quot;susanathey/causalTree&quot;)

library(causalTree)

#setting parameters for the complexity parameters to test for
cp_start = 0
cp_end = 0.0001
cp_steps = 0.00001

#creating sequence of k values to test for
cp_values = seq(from = cp_start, to = cp_end, by = cp_steps)
length(cp_values)

uplift_ct = data.frame(&quot;idx&quot; = data_d$idx, &quot;treatment&quot; = data_d$treatment, &quot;visit&quot; = data_d$visit) 

#calculating uplift for specified min size values
for (cp in cp_values) {
  causal_tree = causalTree(visit~.-spend -conversion -idx -segment.Mens.E.Mail -segment.Womens.E.Mail -segment.No.E.Mail, data = data_d, treatment = data_d$treatment, split.Rule = &quot;CT&quot;, cv.option = &quot;CT&quot;, split.Honest = T, cv.Honest = T, split.Bucket = F, xval = 5, cp = cp, propensity = e_x)
  
  uplift_ct[paste(&quot;uplift_&quot;,cp, sep = &quot;&quot;)] = sapply(causal_tree$where, FUN = function(x){causal_tree$frame$yval[x]})
  
  print(paste(&quot;cp = &quot;, cp))
}

#check result
uplift_ct[1:30, ]

#calculate the transformed outcome
uplift_ct$trans_out = sapply(uplift_ct$idx, FUN = function(x){
  transformed_outcome(e_x = e_x, w_i = data_d$treatment[x], target = data_d$visit[x])
})

#transformed outcome loss
outcome_loss = data.frame(&quot;cp&quot; = cp_values, &quot;loss&quot; = 0)

#find optimal cp value
for (i in 1:length(cp_values)){
  outcome_loss[i, 2] = mean((uplift_ct$trans_out - uplift_ct[, i + 3])^2)
}

outcome_loss

#find minimal value
min(outcome_loss$loss)

outcome_loss$cp[which.min(outcome_loss$loss)]

plot(outcome_loss$loss)

cp_plot = ggplot(data = outcome_loss) +
  geom_point(aes(x = outcome_loss$cp, y = outcome_loss$loss), size = 2, shape = 18) +
  geom_point(aes(x = outcome_loss$cp[which.min(outcome_loss$loss)], y = min(outcome_loss$loss)), 
             size = 4, shape = 18, color = &quot;red&quot;) +
  geom_text(aes(x = outcome_loss$cp[which.min(outcome_loss$loss)], y = min(outcome_loss$loss)), 
            label = paste(&quot;cp = &quot;, outcome_loss$cp[which.min(outcome_loss$loss)]), color = &quot;black&quot;, size = 4, 
            nudge_x = 0.00001, nudge_y = 0, check_overlap = TRUE) +
  stat_smooth(data = outcome_loss, aes(x = outcome_loss$cp, y = outcome_loss$loss), method = &quot;loess&quot;, 
              se = FALSE, span = 0.1) +
  labs(title=&quot;Parameter Optimization of Causal-KNN CATE-Estimation&quot;, x =&quot;Value of Complexity Parameter&quot;, y = &quot;Outcome Loss Value&quot;) +
  theme_light()

cp_plot
</code></pre>

<p><img src="Results/cp_plot.png" alt="Outcome Loss for Different Complexity Parameters of the Causal Tree Model" /></p>

<p>To compare the results of the causal KNN model with the estimations of another uplift model, a causal tree was learned, using the implementation of Susan Athey. The optimal complexity parameter (cp) from the tuning part is used for the CATE predictions of the causal tree.</p>

<pre><code class="language-r,">library(causalTree)

#extracting optimal vlaue for complexity parameter (cp) from tuning
optimal_cp = outcome_loss$cp[which.min(outcome_loss$loss)]

#learning causal tree model for the test set
causal_tree = causalTree(visit~.-spend -conversion -idx -segment.Mens.E.Mail -segment.Womens.E.Mail -segment.No.E.Mail, data = test_set, treatment = test_set$treatment, split.Rule = &quot;CT&quot;, cv.option = &quot;CT&quot;, split.Honest = T, cv.Honest = T, split.Bucket = F, xval = 5, cp = optimal_cp, minsize = 20, propensity = e_x)


causal_tree$frame$yval[1:10]
causal_tree$where[1:10]

max(causal_tree$where)
length(causal_tree$frame$yval)

uplift_ct = data.frame(&quot;idx&quot; = test_set$idx, &quot;treatment&quot; = test_set$treatment, &quot;visit&quot; = test_set$visit) 

uplift_ct$uplift = sapply(causal_tree$where, FUN = function(x){causal_tree$frame$yval[x]})

uplift_ct$uplift[1:10]
causal_tree$where[1:10]
causal_tree$frame$yval[causal_tree$where[1:10]]

head(uplift_ct)
</code></pre>

<h2 id="modell-evaluation">Modell Evaluation</h2>

<p>Vorstellung: Qini-Koeffizient (Max)
Similarities (Qini: vgl. Devriend p.29ff.)
Differences
Possible Advantages
Vorstellung AUUC (Devriendt)
Vorstellung MSE (Hitsch &amp; Misra)
Uplift Metric (vgl. Gutierrez p. 9 ff.)</p>

<h3 id="mse">MSE</h3>

<p>To find the most promising observations to target, the uplift data frame is sorted, according to the the individual CATE estimations. After specifying the percentage of individuals to receive a treatment, it is possible to select the first x% observations from the sorted data frame. For these individuals, a treatment is expected to have the highest impact on the outcome variable. To evaluate the model, Hitsch and Misra (2018) propose the mean squared error as indicator of the model quality. This measure also allows to compare the model with the proposed treatment assignment of other uplift models. Since we have seen that the transformed outcome is an unbiased estimate for the CATE, we can also use this metric to evaluate the model fit. It is shown in Gutierrez and Hitsch, that, under unconfoundedness, the mean squared error based on the transformed outcome is applicable for the comparison of different model fit on a particular data set.</p>

<pre><code class="language-r,">#sorting observations according to uplift values
uplift_sorted = uplift[order(uplift$uplift_301, decreasing = TRUE), ]

#targeting the top fixed percentage of customers(50%)
top_obs = uplift_sorted$idx[1:(0.5*nrow(uplift_sorted))]

evaluation = data.frame(&quot;visit&quot; = test_set$visit)
evaluation$idx = test_set$idx
evaluation$treatment_cknn = 0
evaluation$treatment_cknn[top_obs] = 1

#calculate mean squared error (Hitsch - page 27)
mse = mean((transformed_outcome(e_x = e_x, test_set$treatment, test_set$visit) - uplift$uplift_301)^2)
mse
</code></pre>

<pre><code class="language-r,">###calculating the mse of the causal tree model
#sorting observations according to uplift values
uplift_ct_sorted = uplift_ct[order(uplift_ct$uplift, decreasing = TRUE), ]

#targeting the top fixed percentage of customers(50%)
top_obs = uplift_ct_sorted$idx[1:(0.5*nrow(uplift_ct_sorted))]

evaluation_ct = data.frame(&quot;visit&quot; = test_set$visit)
evaluation_ct$idx = test_set$idx
evaluation_ct$treatment_ct = 0
evaluation_ct$treatment_ct[top_obs] = 1

#calculate mean squared error (Hitsch - page 27)
mse_ct = mean((transformed_outcome(e_x = e_x, test_set$treatment, test_set$visit) - uplift_ct$uplift)^2)
mse_ct

#comparing both mse values
mse
mse_ct
</code></pre>

<h3 id="qini-coefficient-for-evaluation-of-the-causal-knn-model">Qini Coefficient for evaluation of the causal KNN model</h3>

<p>The Gini-Coefficient is a widely used metric to compare the fit of different modeling approaches on a particular data set. The metric is intuitive and applicable, since it provides a single value for the evaluation. The value of the Gini Coefficient is defined between 0 and 1, where 1 represents a perfect model and 0 indicates a fit that is not performing any better than a random ranking of customers. Devriendt states that the Gini-Coefficient, however, is not readily applicable in uplift modeling context, since, for every class, we have individuals from treatment and control group. Therefore in this context, the so called &ldquo;Qini-Coefficient&rdquo; is preferred. The Qini Coefficient is mathematically equal to the difference of the Gini-Curve for the treatment group and the control group.
The qini coefficient provides an opportunity to evaluate uplift models, according to the treatment effect estmations. Since the qini is a common indicator for the quality of uplift models, it is used here to further evaluate the causal KNN model results.</p>

<pre><code class="language-r,">###qini coefficient
#random assignment of treatments
uplift[1:10, ]
qini_data_rnd = data.frame(uplift)
qini_data_rnd$visit = test_set$visit

plot(cumsum(qini_data_rnd$visit))

#incremental gain
segments = seq(0, nrow(qini_data_rnd), by = 100)
incremental_gain = data.frame(&quot;part_from&quot; = segments[1:(length(segments)-1)]+1)
incremental_gain$part_to = segments[1:(length(segments)-1)]+100
incremental_gain$sum = 0

#calculate reactions for segments
for (i in 1:(nrow(incremental_gain))){
  incremental_gain$sum[i] = sum(qini_data_rnd$visit[incremental_gain$part_from[i]:incremental_gain$part_to[i]])
}

#cumulative reactions
incremental_gain$cumsum = cumsum(incremental_gain$sum)
incremental_gain[1:10,]

sum(qini_data_rnd$visit[1:100])
sum(qini_data_rnd$visit)

plot(incremental_gain$cumsum)

as_random = incremental_gain$cumsum


#model assignment of treatments
qini_data_model = qini_data_rnd[order(qini_data_rnd$uplift_301, decreasing = TRUE), ]

plot(cumsum(qini_data_model$visit))

#incremental gain
segments = seq(0, nrow(qini_data_model), by = 100)
incremental_gain = data.frame(&quot;part_from&quot; = segments[1:(length(segments)-1)]+1)
incremental_gain$part_to = segments[1:(length(segments)-1)]+100
incremental_gain$sum = 0

#calculate reactions for segments
for (i in 1:(nrow(incremental_gain))){
  incremental_gain$sum[i] = sum(qini_data_model$visit[incremental_gain$part_from[i]:incremental_gain$part_to[i]])
}

#cumulative reactions
incremental_gain$cumsum = cumsum(incremental_gain$sum)
incremental_gain[1:10,]

sum(qini_data_model$visit[1:100])
sum(qini_data_model$visit)

plot(incremental_gain$cumsum)

as_model = incremental_gain$cumsum

test_set$visit[1:50]
qini_data_model$visit[1:50]

sum(test_set$visit[1:5000])
sum(qini_data_model$visit[1:5000])

#plotting qini curves of the causal knn predictions
qini_data_rnd[1:10, ]
qini_data_model[1:10, ]

qini_plot_data = data.frame(&quot;visit_rnd&quot; = cumsum(qini_data_rnd$visit), &quot;visit_model&quot; = cumsum(qini_data_model$visit), &quot;idx&quot; = 1:nrow(qini_data_rnd))

library(ggplot2)

qini_plot = ggplot(data = qini_plot_data, aes(x = idx)) +
  geom_smooth(aes(y = visit_rnd), method = &quot;lm&quot;, se = FALSE) +
  geom_smooth(aes(y = visit_model), method = &quot;loess&quot;, se = FALSE, span = 2, color = &quot;red&quot;) + 
labs(title = &quot;Qini-Curves of Treatment Assignment&quot;, x = &quot;Observations&quot;, y = &quot;Cumulative Visitations of the Website&quot;) +
  theme_light()
  
max(qini_plot_data$visit_rnd)
max(qini_plot_data$visit_model)
  
qini_plot


#plotting qini curves of the causal tree predictions
#random assignment of treatments
uplift[1:10, ]
qini_data_rnd$uplift_301 = NULL
qini_data_rnd$uplift = uplift_ct$uplift

qini_data_model = qini_data_rnd[order(qini_data_rnd$uplift, decreasing = TRUE), ]

qini_data_rnd[1:10, ]
qini_data_model[1:10, ]

qini_plot_data = data.frame(&quot;visit_rnd&quot; = cumsum(qini_data_rnd$visit), &quot;visit_model&quot; = cumsum(qini_data_model$visit), &quot;idx&quot; = 1:nrow(qini_data_rnd))

library(ggplot2)

qini_plot_ct = ggplot(data = qini_plot_data, aes(x = idx)) +
  geom_smooth(aes(y = visit_rnd), method = &quot;lm&quot;, se = FALSE) +
  geom_smooth(aes(y = visit_model), method = &quot;loess&quot;, se = FALSE, span = 2, color = &quot;red&quot;) + 
labs(title = &quot;Qini-Curves of Treatment Assignment&quot;, x = &quot;Observations&quot;, y = &quot;Cumulative Visitations of the Website&quot;) +
  theme_light()
  
max(qini_plot_data$visit_rnd)
max(qini_plot_data$visit_model)
  
qini_plot_ct
</code></pre>

<p><img src="Results/qini_plot_cknn.png" alt="Qini Curves for the Causal KNN Targeting Policy" /></p>

<p><img src="Results/qini_plot_ct.png" alt="Qini Curves for the Causal Tree Targeting Policy" /></p>

<h3 id="auuc">AUUC</h3>

<p>This measure is defined by the area under the uplift curve. We can compare the resulting uplift curve to the optimal curve. Therefore it has strong relations to the Qini-Curve, but this measure does not take the incremental effect of random targeting into account.</p>

<table>
<thead>
<tr>
<th>Model</th>
<th>MSE</th>
<th>AUUC</th>
</tr>
</thead>

<tbody>
<tr>
<td>Causal KNN (K = 250)</td>
<td>0.5914</td>
<td></td>
</tr>

<tr>
<td>Causal Tree</td>
<td>0.5803</td>
<td></td>
</tr>
</tbody>
</table>

<h2 id="conclusion">Conclusion</h2>

                        </div>
                        
                        

                    </div>
                    

                    

                    

                    <div class="col-md-3">

                        

                        








<div class="panel panel-default sidebar-menu">
    <div class="panel-heading">
      <h3 class="panel-title">Categories</h3>
    </div>

    <div class="panel-body">
        <ul class="nav nav-pills nav-stacked">
            
            <li><a href="https://humboldt-wi.github.io/blog/categories/course-projects">course-projects (19)</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/categories/instruction">instruction (2)</a>
            </li>
            
        </ul>
    </div>
</div>











<div class="panel sidebar-menu">
    <div class="panel-heading">
      <h3 class="panel-title">Tags</h3>
    </div>

    <div class="panel-body">
        <ul class="tag-cloud">
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/attention"><i class="fa fa-tags"></i> attention</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/awd-lstm"><i class="fa fa-tags"></i> awd-lstm</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/bayesian-deep-learning"><i class="fa fa-tags"></i> bayesian-deep-learning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/bayesian-topic-modelling"><i class="fa fa-tags"></i> bayesian-topic-modelling</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/black-box"><i class="fa fa-tags"></i> black-box</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/blockchain"><i class="fa fa-tags"></i> blockchain</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/causal-inference"><i class="fa fa-tags"></i> causal-inference</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/class17/18"><i class="fa fa-tags"></i> class17/18</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/class18/19"><i class="fa fa-tags"></i> class18/19</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/class19"><i class="fa fa-tags"></i> class19</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/classification"><i class="fa fa-tags"></i> classification</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/conversion"><i class="fa fa-tags"></i> conversion</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/convolutional-neural-networks"><i class="fa fa-tags"></i> convolutional-neural-networks</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/deep-learning"><i class="fa fa-tags"></i> deep-learning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/distant-transfer-learning"><i class="fa fa-tags"></i> distant-transfer-learning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/doc2vec"><i class="fa fa-tags"></i> doc2vec</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/document-embeddings"><i class="fa fa-tags"></i> document-embeddings</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/explanation"><i class="fa fa-tags"></i> explanation</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/fine-tuning"><i class="fa fa-tags"></i> fine-tuning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/gru"><i class="fa fa-tags"></i> gru</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/hierarchical-network"><i class="fa fa-tags"></i> hierarchical-network</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/ice"><i class="fa fa-tags"></i> ice</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/image-analysis"><i class="fa fa-tags"></i> image-analysis</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/image-captioning"><i class="fa fa-tags"></i> image-captioning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/inference"><i class="fa fa-tags"></i> inference</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/keras-imdb-dataset"><i class="fa fa-tags"></i> keras-imdb-dataset</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/knn-algorithm"><i class="fa fa-tags"></i> knn-algorithm</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/language-modelling"><i class="fa fa-tags"></i> language-modelling</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/lda"><i class="fa fa-tags"></i> lda</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/lime"><i class="fa fa-tags"></i> lime</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/long-short-term-memory"><i class="fa fa-tags"></i> long-short-term-memory</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/lstm"><i class="fa fa-tags"></i> lstm</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/monte-carlo-dropout"><i class="fa fa-tags"></i> monte-carlo-dropout</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/movie-reviews"><i class="fa fa-tags"></i> movie-reviews</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/neural-network"><i class="fa fa-tags"></i> neural-network</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/neural-networks"><i class="fa fa-tags"></i> neural-networks</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/nlp"><i class="fa fa-tags"></i> nlp</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/pdp"><i class="fa fa-tags"></i> pdp</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/pretraining"><i class="fa fa-tags"></i> pretraining</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/recommendation"><i class="fa fa-tags"></i> recommendation</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/recommender-system"><i class="fa fa-tags"></i> recommender-system</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/rnn"><i class="fa fa-tags"></i> rnn</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/sentiment-analysis"><i class="fa fa-tags"></i> sentiment-analysis</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/sentiment-classification"><i class="fa fa-tags"></i> sentiment-classification</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/share-price-prediction"><i class="fa fa-tags"></i> share-price-prediction</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/text-analysis"><i class="fa fa-tags"></i> text-analysis</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/text-mining"><i class="fa fa-tags"></i> text-mining</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/time-series"><i class="fa fa-tags"></i> time-series</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/time-series-forecasting"><i class="fa fa-tags"></i> time-series-forecasting</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/transfer-learning"><i class="fa fa-tags"></i> transfer-learning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/twitter"><i class="fa fa-tags"></i> twitter</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/ulmfit"><i class="fa fa-tags"></i> ulmfit</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/uplift"><i class="fa fa-tags"></i> uplift</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/uplift-modelling"><i class="fa fa-tags"></i> uplift-modelling</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/variational-inference"><i class="fa fa-tags"></i> variational-inference</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/wikitext-103"><i class="fa fa-tags"></i> wikitext-103</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/word-embeddings"><i class="fa fa-tags"></i> word-embeddings</a>
            </li>
            
        </ul>
    </div>
</div>



















                        

                    </div>
                    

                    

                </div>
                

            </div>
            
        </div>
        

        <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<footer id="footer">
    <div class="container">

        

        <div class="col-md-4 col-sm-6">

             
            

            
                
                
                    
                        
                          
                            
                          
                        
                    
                    
                        
                    
                
                
                
                    
                        
                          
                            
                          
                        
                    
                    
                        
                    
                
                
                
                    
                        
                          
                            
                          
                        
                    
                    
                        
                    
                
                
            

            <hr class="hidden-md hidden-lg">
             

        </div>
        

        

    </div>
    
</footer>







<div id="copyright">
    <div class="container">
        <div class="col-md-12">
            
            <p class="pull-left">Copyright (c) 2017, Chair of Information System at HU-Berlin; all rights reserved.</p>
            
            <p class="pull-right">
              Template by <a href="http://bootstrapious.com/free-templates">Bootstrapious</a>.
              

              Ported to Hugo by <a href="https://github.com/devcows/hugo-universal-theme">DevCows</a>
            </p>
        </div>
    </div>
</div>





    </div>
    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-112025566-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

<script src="//code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/waypoints/4.0.1/jquery.waypoints.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0/jquery.counterup.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-parallax/1.1.3/jquery-parallax.js"></script>

<script src="//maps.googleapis.com/maps/api/js?v=3.exp"></script>

<script src="https://humboldt-wi.github.io/blog/js/hpneo.gmaps.js"></script>
<script src="https://humboldt-wi.github.io/blog/js/gmaps.init.js"></script>
<script src="https://humboldt-wi.github.io/blog/js/front.js"></script>


<script src="https://humboldt-wi.github.io/blog/js/owl.carousel.min.js"></script>


  </body>
</html>
